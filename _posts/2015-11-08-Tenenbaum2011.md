---
layout: post
title:  "How to grow a mind: statistics, structure, and abstraction"
date:   2015-11-08 18:11:51
categories: probabilistic-models
---


{% reference Tenenbaum2011 %}

# Summary

Tenenbaum et al. attempt to address three questions:

1. How does abstract knowledge guide learning and inference from sparse data?
2. What forms does abstract knowledge take, across different domains and tasks?
3. How is abstract knowledge itself acquired?

The overarching answer to these three questions is "hierarchical Bayesian models" (HBMs):

1. "Abstract knowledge is encoded in a probabilistic generative model, a kind of mental model that describes the causal processes in the world giving rise to the learner's observations as well as unobserved or latent variables that support effective prediction and action if the learner can infer their hidden state."
2. Abstract knowledge takes the form of structured, symbolic representations, such as "graphs, grammars, predicate logic, relational schemas, and functional programs". The form of the knowledge itself can also be inferred via probabilistic generative models, through the use of "reltional data structures such as graph schemas, templates for graphs based on types of nodes, or probabilistic graph grammars".
3. This is really where HBMs come into play: they "address the origins of hypothesis spaces and priors by positing not just a single level of hypotheses to explain the data but multiple levels: hypothesis spaces of hypothesis spaces, with priors on priors".

HBMs are so powerful because they simultaneously provide constraints on what types of things can be learned, while also allowing for flexbility in what they can be applied to. For example, Tenenbaum et al. discuss the case of inferring which diseases cause which symptoms. In the simpler case, the task is to learn which nodes in the probabilistic graphical model have edges between them. In the HBM case, there is the additional constraint of assuming two classes of nodes (diseases and symptoms) and that there is a preference for edges to go from diseases to symptoms. Inference is then performed over which nodes are diseases and which are symptoms, as well as what the edges are. The introduction of this additional structure makes inference much faster and accurate. However, the model isn't limited just to this medical case; it could apply to any similar bipartite structure.

There is an important point about contrast to previous nativist vs empiricist approaches:

* In the nativist case, researchers assumed anything important was innately specified, and that very little learning happened. They used sophisticated, structured knowledge (like logic) but it had very little ability to generalize because it lacked a mechanism for learning.
* In the empiricist case, researchers assumed very little to be specified innately, and that anything could be learned. They used powerful models of learning (like connectionist models), but the representations that those models used were flat and simplistic.

PGMs find a middle ground between these two approaches, allowing both for powerful learning mechanisms as well as sophisticated, structured representations of knowledge.

# Methods

n/a

# Algorithm

n/a

# Takeaways

Structured forms of knowledge instantiated in hierarchical Bayesian models give us a framework for explaining how people get so much from so little. They have been successfully used to model many simple forms of abstract knowledge, though it remains to be seen how they can explain how we learn broad framework theories for topics such as intuitive physics or theory of mind. It also remains to be seen how these types of structure knowledge representations can be computed neurally.
